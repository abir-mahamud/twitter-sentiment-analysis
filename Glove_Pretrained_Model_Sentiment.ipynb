{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove Pretrained Model Sentiment.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffeWqPaZSlmJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "import keras\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "\n",
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
        "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwaN2M8YTHhK"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGTtLHSqTXGr"
      },
      "source": [
        "# Load dataset\n",
        "def load_data():\n",
        "    data =pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Data/tweet_covid_processing_file2.csv\", engine='python')\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Jen107Ta3i"
      },
      "source": [
        "tweet_df = load_data()\n",
        "tweet_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-XxRqkLTeQg"
      },
      "source": [
        "tweet_df.drop(['Unnamed: 0','OriginalTweet','Sentiment','tweet_token','tweet_token_filtered'],inplace=True,axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nmW8W-QTgtx"
      },
      "source": [
        "tweet_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4g7yOyOdqCn"
      },
      "source": [
        "tweet_df['tweet_lemmatized'] = tweet_df['tweet_lemmatized'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DuhqPj6cuRg"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7OvnuhocuU2"
      },
      "source": [
        "X = tweet_df['tweet_lemmatized']\n",
        "y = tweet_df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je_TTdB2ekOa"
      },
      "source": [
        "X.shape,y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EamgVh9gcubh"
      },
      "source": [
        "#50328 only for glove\n",
        "#2000001 only for Fasttext\n",
        "max_features=2000001\n",
        "max_len=100\n",
        "sequence_input = Input(shape=(max_len, ))\n",
        "embed_size=300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoLqwUjKcueX"
      },
      "source": [
        "tokenizer = text.Tokenizer(num_words = max_features,lower = True\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(X)\n",
        "X=tokenizer.texts_to_sequences(X)\n",
        "X=sequence.pad_sequences(X,maxlen=max_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bOh1CAXfSqr"
      },
      "source": [
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKOEnrxpm-Gr"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "counter = Counter(y)\n",
        "for k,v in counter.items():\n",
        "\tper = v / len(y) * 100\n",
        "\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
        "# plot the distribution\n",
        "pyplot.bar(counter.keys(), counter.values())\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsY-cMOnfSi0"
      },
      "source": [
        "X.shape,y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANIKIhavUIwA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP-yw7XCUajx"
      },
      "source": [
        "glove_file = '/content/drive/My Drive/Twiter Sentiment Analysis/Data/glove.6B.100d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDlPgA58eyJf"
      },
      "source": [
        "glove_6B_100d_index = {}\n",
        "with open(glove_file, encoding='utf8') as file:\n",
        "    for line in file:\n",
        "        values = line.rstrip().rsplit(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        glove_6B_100d_index[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POKcVhpteqk_"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "#prepare embedding matrix\n",
        "num_words = min(max_features, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features:\n",
        "        continue\n",
        "    embedding_vector = glove_6B_100d_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGEnzJtQhZ_f"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWdIb-aHnxo8"
      },
      "source": [
        "# LSTM Implimentation\n",
        "from tensorflow.keras.layers import Embedding,SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers.convolutional import Conv1D  \n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Bidirectional, Dense,Dropout\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras import layers\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td4f-eRTnhVd"
      },
      "source": [
        "model2=Sequential()\n",
        "model2.add(Embedding(max_features,100,weights=[embedding_matrix],trainable=True ))\n",
        "model2.add(SpatialDropout1D(0.4))\n",
        "model2.add(Bidirectional(LSTM(128)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(5,activation='softmax'))\n",
        "#model2.add(Dense(1,activation='relu'))\n",
        "adam = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model2.compile(loss='sparse_categorical_crossentropy',optimizer=adam,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gRcHiexnhR6"
      },
      "source": [
        "history = model2.fit(X_train,y_train,batch_size=128,epochs=5,validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ95kmnlnhLS"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Bidirectional, Dense,Dropout\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.layers import Dropout\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "cvscores = []\n",
        "\n",
        "for train, test in kfold.split(X,y):\n",
        "    ## Creating model\n",
        "    model2=Sequential()\n",
        "    model2.add(Embedding(max_features,100,weights=[embedding_matrix],trainable=True ))\n",
        "    model2.add(SpatialDropout1D(0.4))\n",
        "    model2.add(Bidirectional(LSTM(128)))\n",
        "    model2.add(Dropout(0.2))\n",
        "    model2.add(Dense(5,activation='softmax'))\n",
        "    #model2.add(Dense(1,activation='relu'))\n",
        "    adam = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "    model2.compile(loss='sparse_categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
        "    # Fit the model\n",
        "    history = model2.fit(X[train], y[train],validation_data=(X_test, y_test),epochs=10,batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7A02czT4N3s"
      },
      "source": [
        "hist_df = pd.DataFrame(history.history)\n",
        "hist_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py29Axhm4YQn"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(hist_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(hist_df['accuracy'], label='Training Accuracy')\n",
        "#plt.axvline(x = 4, linewidth=1, color='r', linestyle = \"--\")\n",
        "\n",
        "plt.title('Training and Validation Accuracy with Glove Pretrained Model')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(hist_df['loss'], label='Training Loss')\n",
        "plt.plot(hist_df['val_loss'], label='Validation Loss')\n",
        "#plt.axvline(x = 4, linewidth=1, color='r', linestyle = \"--\")\n",
        "\n",
        "plt.title('Training and Validation Loss with Glove Pretrained Model')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAgv04nosTYx"
      },
      "source": [
        "y_pred=model2.predict_classes(X_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test,y_pred)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw_dAj5CBwx3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y60rfuS8Am7"
      },
      "source": [
        "Fast Text Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84bJipcf75gH"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "import gzip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxTGcHVr75qf"
      },
      "source": [
        "file = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4UcIcz475t-"
      },
      "source": [
        "vocab_and_vectors = {}  \n",
        "for line in file:\n",
        "  values = line.split()\n",
        "  word = values[0].decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK_qEkBh75yB"
      },
      "source": [
        "embedding_matrix = np.zeros((len(vocab_and_vectors) + 1, 300))\n",
        "for i, word, in enumerate(vocab_and_vectors.keys()):\n",
        "  embedding_vector = vocab_and_vectors.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L26GEmGt754G"
      },
      "source": [
        "model5=Sequential()\n",
        "model5.add(Embedding(max_features,300,weights=[embedding_matrix],trainable=True ))\n",
        "model5.add(SpatialDropout1D(0.4))\n",
        "model5.add(Bidirectional(LSTM(128)))\n",
        "model5.add(Dropout(0.2))\n",
        "model5.add(Dense(5,activation='softmax'))\n",
        "#model2.add(Dense(1,activation='relu'))\n",
        "adam = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model5.compile(loss='sparse_categorical_crossentropy',optimizer=adam,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hvx7wwD-kyI"
      },
      "source": [
        "[1]*10**10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ7qqjU2756-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BWBr0oo7598"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CRAyYdX76Bf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvn_uD3K76Eh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAeLH7wdfzRR"
      },
      "source": [
        "layer = Embedding(max_features, \n",
        "                  embed_size, \n",
        "                  weights=[embedding_matrix], \n",
        "                  trainable = True\n",
        "                  )(sequence_input)\n",
        "\n",
        "layer = SpatialDropout1D(0.2)(layer)\n",
        "\n",
        "layer = Bidirectional(GRU(128, \n",
        "                          return_sequences=True, \n",
        "                          dropout=0.2, \n",
        "                          recurrent_dropout=0.2)\n",
        "                      )(layer)\n",
        "\n",
        "layer = Conv1D(64, kernel_size = 3, \n",
        "               padding = \"valid\", \n",
        "               kernel_initializer = \"glorot_uniform\"\n",
        "               )(layer)\n",
        "\n",
        "avg_pool = GlobalAveragePooling1D()(layer)\n",
        "\n",
        "max_pool = GlobalMaxPooling1D()(layer)\n",
        "\n",
        "layer = concatenate([avg_pool, max_pool]) \n",
        "\n",
        "preds = Dense(5, activation=\"sigmoid\")(layer)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaIouwiLgZ9C"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-_GDXAogkz8"
      },
      "source": [
        "history = model.fit(X_train,y_train,batch_size=128,epochs=5,validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0t2BivPyIlM"
      },
      "source": [
        "hist_df = pd.DataFrame(history.history)\n",
        "hist_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMHeGq8MyXkr"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(hist_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(hist_df['accuracy'], label='Training Accuracy')\n",
        "plt.axvline(x = 4, linewidth=1, color='r', linestyle = \"--\")\n",
        "\n",
        "plt.title('Multilabel Classification Training and Validation')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}